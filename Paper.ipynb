{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969520ed",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "\n",
    "In this research, EEG data was studied. Data Set was retrieved from https://physionet.org/content/eegmat/1.0.0/. The data was transferred to Jupyter Notebook via the folder downloaded to local. The work was done on a Jupyter Notebook.\n",
    "\n",
    "The libraries used in the Python environment are listed below;\n",
    "\n",
    "1-) **glob**: The recorded signals consist of 70 files with .edf extension. The glob module in python was used to be able to read these files. Glob module helps to list specific files in folder.\n",
    "\n",
    "2-) **os**: The os module provides support for using files and folders between operating systems.\n",
    "\n",
    "3-) **mne**: It is a python package that allows working with EEG and MEG data. It allows to read EEG data.\n",
    "\n",
    "4-) **numpy**: Numpy is a library that facilitates mathematical operations on large arrays and arrays.\n",
    "\n",
    "5-) **pandas**: Pandas is used to process, analyze and manipulate data.\n",
    "\n",
    "6-) **matplotlib.pyplot**: It is used to visualize data.\n",
    "\n",
    "## Data\n",
    "\n",
    "There are 72 files in the folder. There are 36 people who participated in the study. Each person has 2 files, before and during the arithmetic test. These files are kept with the .edf extension. \n",
    "\n",
    "## Data Manipulation\n",
    "With the glob library, files were transferred to Jupyter Notebook as a list. After creating 2 lists named **before_test** and **during_test**, the data was converted to these two lists. For this process, split function and comprehension structure were used. \n",
    "\n",
    "After the files were transferred to the lists, the files of a trial subject before and after the arithmetic test were examined. For this process, the io.read_raw_edf function of the mne library was used. This process was repeated for several subjects. According to the results, the signals were collected from 21 channels. Before arithmetic test data was observed as approximately 3 times the time of after arithmetic test data. \n",
    "\n",
    "After these results, the **read_data** function was created to operate on all data. The io.read_raw_edf function is used inside the created function. The **filter** function was used to filter the frequency values. Afterwards, the signals in each channel were split into 5-second epochs using the **make_fixed_length_epochs** function. By adding the **overlap parameter** to the function, the last 1 second of the previous epoch and the first 1 second of the next epoch are overlapped. Finally, the data is converted to array using **get_data** function.\n",
    "\n",
    "This operation changed the structure of the data. The 2-dimensional data consisting of 21 channels and the signals in each channel turned into a 3-dimensional data. The data from each experiment was inserted into this function and placed in 2 lists named **before_epochs_list** and **during_epochs_list**. Later, **before_epochs_labels** and **during_epochs_labels** lists were created from the lists created in the previous step to use when estimating. While 0 label was assigned before the arithmetic test, 1 label was assigned to the data collected during the arithmetic process. Arrays created for labeling are one-dimensional arrays. Also at this stage, the group_label array was created, with labels given to each of the 72 experiments. However, as will be understood in the next steps, using this array as the dependent variable caused the model to be overfit.\n",
    "\n",
    "**Data_list** is created by adding before_epochs_array and during_epochs_array end-to-end. The same action was applied to the before_epochs_labels and during_epochs_labels lists to create the **label_list**. At this point, the shapes of the formed lists can be explained. data_list is a list that holds arrays with epochs from each search. Likewise, label_list holds the labeled values for each search. Therefore, it is necessary to add each item in these lists together. **vstack** and **hstack** functions are used for this task. While vstack is a function that adds arrays vertically, hstack is a function that adds arrays horizontally.  For this reason, vstack is used while performing this operation in the data_list function. In the last case, the names of our resulting arrays are **data_array** and **list_array**. data_array is a 3 dimensional array while list_array is a one dimensional array. \n",
    "\n",
    "### Prediction\n",
    "\n",
    "Before starting the estimation, the numbers \"1\" and \"0\" in the dependent variable were examined using the Counter module. In order to train the model and measure its accuracy, the data was divided into two parts as train and test. For this process, train_test_split function is used. The function is given data_array as the independent variable while the dependent variable is label_array. The size of the test data is set to 20%. Since the data array is 3-dimensional, it is necessary to arrange the shapes after the data is separated into train and test. While the first dimension with the epochs remained the same, the 2nd and 3rd dimensions were multiplied, reducing the argument array to 2 dimensions. \n",
    "\n",
    "Firstly, Logistic Regression was used while estimating. It was decided to use the **liblinear** hyperparameter. After running the model, score and confusion matrix were examined. SVM was also applied to the dataset with all subjects.  In the SVM model, **rbf** was chosen as the hyperparameter and the C value was given as 2.5. Confusion matrix and score were checked.\n",
    "\n",
    "### Other Function \n",
    "\n",
    "--It was observed that taking all subjects together produces a constant model.-- For this reason, it was designed to make predictions for the subjects one by one. The above-mentioned file reading, merging, editing and modeling operations are organized in a single function called read_data. The function reads a subject's file with the method taken from the mne library, then assigns it to the list before and after the task. The data before the task is labeled as 0 and the data after the task as 1. It is then divided into epochs as explained in more detail above. It is split into array_data and label_data. After these processes, the PCA method was tried because the number of signal channels was high. The PCA method was adapted to the data with transform, fit methods, to obtain explainability with 97 percent variance. The data was divided into two as test data 20 percent and train data 80 percent. The support vector machine(SVM) model was created with the kernel as rbf and the C value as two. The X_train and y_train data were separated earlier were fitted to the model. y_pred is created prediction values with x_test. Confusion matrix is printed. In order to observe whether PCA has an effect on the model, the PCA code lines were removed and the same model was constructed. After the Support vector machine model, the logistic regression model established above was integrated into the function to be controlled in individual subjects. Again, the confusion matrix and score result was observed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf6654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
